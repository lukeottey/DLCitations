\documentclass[acmlarge]{acmart}
\AtBeginDocument{%
\providecommand\BibTeX{{%
\normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}
\usepackage{setspace}
\author{Luke Ottey} 
\begin{document}
\begin{center}
\textbf{Deep Learning and Computer Vision Papers}
\end{center}
Luke Ottey
\newline
\href{mailto:lottey98@gmail.com}{\nolinkurl{lottey98@gmail.com}} 
\singlespacing
\tableofcontents
\newpage
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{architectures}
	\subsection{first deep CNNs}
	\begin{enumerate}
		\item
	\end{enumerate}
	\subsection{building blocks}
		\subsubsection{residual connections}
		\begin{enumerate}
			\item
		\end{enumerate}
		\subsubsection{dense connections}
		\begin{enumerate}
			\item
		\end{enumerate}
		\subsubsection{inception-style}
		\begin{enumerate}
			\item Xception: Deep Learning with Depthwise Separable Convolutions \cite{Chollet2017XceptionDL} 

		\end{enumerate}
		\subsubsection{drop-in and add-on components}
		\begin{enumerate}
			\item
		\end{enumerate}
	\begin{enumerate}
	\end{enumerate}
	\subsection{efficiency-oriented}
		\subsubsection{mobile vision}
		\begin{enumerate}
			\item
		\end{enumerate}
		\subsubsection{model scaling}
		\begin{enumerate}
			\item Scale-Aware Trident Networks for Object Detection \cite{Li2019ScaleAwareTN} 

		\end{enumerate}
	\begin{enumerate}
		\item ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design \cite{Ma2018ShuffleNetVP} 

		\item Rethinking Channel Dimensions for Efficient Model Design \cite{Han2021RethinkingCD} 

	\end{enumerate}
	\subsection{attention mechanisms}
	\begin{enumerate}
		\item Self-Supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation \cite{Wang2020SelfSupervisedEA} 

	\end{enumerate}
	\subsection{neural architecture search}
	\begin{enumerate}
		\item Searching for Activation Functions \cite{Ramachandran2018SearchingFA} 

	\end{enumerate}
	\subsection{other layers}
	\begin{enumerate}
		\item A ConvNet for the 2020s \cite{Liu2022ACF} 

	\end{enumerate}
	\subsection{transformers}
	\begin{enumerate}
		\item Vision Transformers for Dense Prediction \cite{Ranftl2021VisionTF} 

		\item LoFTR: Detector-Free Local Feature Matching with Transformers \cite{Sun2021LoFTRDL} 

		\item Transformers in Vision: A Survey \cite{Khan2022TransformersIV} 

		\item Spatial Transformer Networks \cite{Jaderberg2015SpatialTN} 

	\end{enumerate}
\begin{enumerate}
	\item Revisiting ResNets: Improved Training and Scaling Strategies \cite{Bello2021RevisitingRI} 

	\item ResNeSt: Split-Attention Networks \cite{Zhang2020ResNeStSN} 

	\item i-RevNet: Deep Invertible Networks \cite{Jacobsen2018iRevNetDI} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{regularization}
	\subsection{augmentations}
		\subsubsection{mixed-sampling}
		\begin{enumerate}
			\item
		\end{enumerate}
		\subsubsection{automatic}
		\begin{enumerate}
			\item
		\end{enumerate}
	\begin{enumerate}
		\item A Simple Framework for Contrastive Learning of Visual Representations \cite{Chen2020ASF} 

		\item What makes for good views for contrastive learning \cite{Tian2020WhatMF} 

		\item TrivialAugment: Tuning-free Yet State-of-the-Art Data Augmentation \cite{Mller2021TrivialAugmentTY} 

		\item Unsupervised Representation Learning by Predicting Image Rotations \cite{Gidaris2018UnsupervisedRL} 

		\item Understanding Data Augmentation for Classification: When to Warp? \cite{Wong2016UnderstandingDA} 

		\item Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation \cite{Ghiasi2021SimpleCI} 

	\end{enumerate}
	\subsection{consistency}
	\begin{enumerate}
		\item A Simple Framework for Contrastive Learning of Visual Representations \cite{Chen2020ASF} 

		\item Unsupervised Data Augmentation for Consistency Training \cite{Xie2020UnsupervisedDA} 

	\end{enumerate}
	\subsection{knowledge-distillation}
	\begin{enumerate}
		\item WSOD2: Learning Bottom-Up and Top-Down Objectness Distillation for Weakly-Supervised Object Detection \cite{Zeng2019WSOD2LB} 

		\item Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection \cite{Huang2020ComprehensiveAS} 

		\item Unbiased Teacher for Semi-Supervised Object Detection \cite{Liu2021UnbiasedTF} 

	\end{enumerate}
	\subsection{intermediate-layer}
	\begin{enumerate}
		\item
	\end{enumerate}
	\subsection{pseudo-labeling}
	\begin{enumerate}
		\item SPICE: Semantic Pseudo-labeling for Image Clustering \cite{Niu2021SPICESP} 

	\end{enumerate}
\begin{enumerate}
	\item When Does Label Smoothing Help? \cite{Mller2019WhenDL} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{activation}
	\subsection{layers}
	\begin{enumerate}
		\item
	\end{enumerate}
\begin{enumerate}
	\item Searching for Activation Functions \cite{Ramachandran2018SearchingFA} 

	\item Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds \cite{Chen2019EfficientAO} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{normalization}
	\subsection{layers}
	\begin{enumerate}
		\item
	\end{enumerate}
	\subsection{self-normalization}
	\begin{enumerate}
		\item Self-Normalizing Neural Networks \cite{Klambauer2017SelfNormalizingNN} 

	\end{enumerate}
\begin{enumerate}
	\item Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks \cite{Salimans2016WeightNA} 

	\item Rethinking "Batch" in BatchNorm \cite{Wu2021RethinkingI} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{adversarial}
\begin{enumerate}
	\item Robust and Accurate Object Detection via Adversarial Learning \cite{Chen2021RobustAA} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{generative modeling}
	\subsection{theory}
	\begin{enumerate}
		\item A Primal-Dual link between GANs and Autoencoders \cite{Husain2019APL} 

	\end{enumerate}
	\subsection{autoencoders}
		\subsubsection{variational}
		\begin{enumerate}
			\item
		\end{enumerate}
		\subsubsection{theory}
		\begin{enumerate}
			\item
		\end{enumerate}
	\begin{enumerate}
		\item Deep Spectral Clustering Using Dual Autoencoder Network \cite{Yang2019DeepSC} 

	\end{enumerate}
	\subsection{generative adversarial networks}
		\subsubsection{theory}
		\begin{enumerate}
			\item
		\end{enumerate}
		\subsubsection{wasserstein}
		\begin{enumerate}
			\item Banach Wasserstein GAN \cite{Adler2018BanachWG} 

			\item Wasserstein GAN \cite{Arjovsky2017WassersteinG} 

		\end{enumerate}
		\subsubsection{maximum mean discrepancy}
		\begin{enumerate}
			\item
		\end{enumerate}
		\subsubsection{conditional/auxiliary}
		\begin{enumerate}
			\item A Multi-Class Hinge Loss for Conditional GANs \cite{Kavalerov2021AMH} 

			\item cGANs with Projection Discriminator \cite{Miyato2018cGANsWP} 

			\item Conditional Image Synthesis with Auxiliary Classifier GANs \cite{Odena2017ConditionalIS} 

			\item Plug \& Play Generative Networks: Conditional Iterative Generation of Images in Latent Space \cite{Nguyen2017PlugP} 

		\end{enumerate}
		\subsubsection{image-to-image translation}
		\begin{enumerate}
			\item
		\end{enumerate}
		\subsubsection{style}
		\begin{enumerate}
			\item Analyzing and Improving the Image Quality of StyleGAN \cite{Karras2020AnalyzingAI} 

			\item Neural Photo Editing with Introspective Adversarial Networks \cite{Brock2017NeuralPE} 

		\end{enumerate}
		\subsubsection{other popular variants}
		\begin{enumerate}
			\item
		\end{enumerate}
	\begin{enumerate}
		\item Disconnected Manifold Learning for Generative Adversarial Networks \cite{Khayatkhoei2018DisconnectedML} 

		\item Do GANs actually learn the distribution? An empirical study \cite{Arora2017DoGA} 

		\item Effect of the Latent Structure on Clustering With GANs \cite{Mishra2020EffectOT} 

		\item DeLiGAN: Generative Adversarial Networks for Diverse and Limited Data \cite{Gurumurthy2017DeLiGANGA} 

		\item Geometric GAN \cite{Lim2017GeometricG} 

		\item Gradient descent GAN optimization is locally stable \cite{Nagarajan2017GradientDG} 

		\item Improved Training of Wasserstein GANs \cite{Gulrajani2017ImprovedTO} 

		\item Is Generator Conditioning Causally Related to GAN Performance? \cite{Odena2018IsGC} 

		\item Large Scale GAN Training for High Fidelity Natural Image Synthesis \cite{Brock2019LargeSG} 

		\item ClusterGAN : Latent Space Clustering in Generative Adversarial Networks \cite{Mukherjee2019ClusterGANL} 

		\item On the discrimination-generalization tradeoff in GANs \cite{zhang2017discrimination} 

		\item On Noise Injection in Generative Adversarial Networks \cite{Feng2020OnNI} 

		\item On Stabilizing Generative Adversarial Training With Noise \cite{Jenni2019OnSG} 

		\item Towards Principled Methods for Training Generative Adversarial Networks \cite{Arjovsky2017TowardsPM} 

		\item MGAN: Training Generative Adversarial Nets with Multiple Generators \cite{Hoang2018MGANTG} 

		\item The Numerics of GANs \cite{Mescheder2017TheNO} 

		\item Twin Auxilary Classifiers GAN \cite{Gong2019TwinAC} 

	\end{enumerate}
\begin{enumerate}
	\item Learning in Implicit Generative Models \cite{Mohamed2016LearningII} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{detection}
	\subsection{detectors}
		\subsubsection{one-stage}
			\paragraph{yolo}
			\begin{enumerate}
				\item YOLACT: Real-Time Instance Segmentation \cite{Bolya2019YOLACTRI} 

				\item YOLOX: Exceeding YOLO Series in 2021 \cite{Ge2021YOLOXEY} 

				\item You Only Look One-level Feature \cite{Chen2021YouOL} 

				\item PP-YOLOv2: A Practical Object Detector \cite{Huang2021PPYOLOv2AP} 

			\end{enumerate}
			\paragraph{retinanet}
			\begin{enumerate}
				\item
			\end{enumerate}
			\paragraph{anchor-free}
			\begin{enumerate}
				\item CenterMask: Real-Time Anchor-Free Instance Segmentation \cite{Lee2020CenterMaskRA} 

			\end{enumerate}
		\begin{enumerate}
		\end{enumerate}
		\subsubsection{two-stage}
			\paragraph{rcnn}
			\begin{enumerate}
				\item
			\end{enumerate}
		\begin{enumerate}
		\end{enumerate}
		\subsubsection{transformers}
		\begin{enumerate}
			\item
		\end{enumerate}
	\begin{enumerate}
	\end{enumerate}
	\subsection{backbones}
	\begin{enumerate}
		\item SpineNet: Learning Scale-Permuted Backbone for Recognition and Localization \cite{Du2020SpineNetLS} 

	\end{enumerate}
	\subsection{anchor box assignment}
	\begin{enumerate}
		\item Region Proposal by Guided Anchoring \cite{Wang2019RegionPB} 

		\item Probabilistic Anchor Assignment with IoU Prediction for Object Detection \cite{Kim2020ProbabilisticAA} 

	\end{enumerate}
	\subsection{segmentation}
		\subsubsection{instance}
		\begin{enumerate}
			\item YOLACT: Real-Time Instance Segmentation \cite{Bolya2019YOLACTRI} 

			\item Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation \cite{Ghiasi2021SimpleCI} 

			\item Semantic Instance Segmentation with a Discriminative Loss Function \cite{Brabandere2017SemanticIS} 

			\item CenterMask: Real-Time Anchor-Free Instance Segmentation \cite{Lee2020CenterMaskRA} 

		\end{enumerate}
		\subsubsection{semantic}
		\begin{enumerate}
			\item
		\end{enumerate}
		\subsubsection{panoptic}
		\begin{enumerate}
			\item
		\end{enumerate}
	\begin{enumerate}
		\item Simple Does It: Weakly Supervised Instance and Semantic Segmentation \cite{Khoreva2017SimpleDI} 

		\item Weakly Supervised Learning of Instance Segmentation With Inter-Pixel Relations \cite{Ahn2019WeaklySL} 

		\item TensorMask: A Foundation for Dense Object Segmentation \cite{Chen2019TensorMaskAF} 

		\item The surprising impact of mask-head architecture on novel class segmentation \cite{Birodkar2021TheSI} 

		\item Unsupervised Object Segmentation by Redrawing \cite{Chen2019UnsupervisedOS} 

		\item Self-Supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation \cite{Wang2020SelfSupervisedEA} 

	\end{enumerate}
	\subsection{weakly-supervised learning}
	\begin{enumerate}
		\item WSOD2: Learning Bottom-Up and Top-Down Objectness Distillation for Weakly-Supervised Object Detection \cite{Zeng2019WSOD2LB} 

		\item Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection \cite{Huang2020ComprehensiveAS} 

		\item Self-Supervised Equivariant Attention Mechanism for Weakly Supervised Semantic Segmentation \cite{Wang2020SelfSupervisedEA} 

	\end{enumerate}
	\subsection{semi-supervised learning}
	\begin{enumerate}
		\item Unbiased Teacher for Semi-Supervised Object Detection \cite{Liu2021UnbiasedTF} 

	\end{enumerate}
	\subsection{applied}
	\begin{enumerate}
		\item Depth CNNs for RGB-D Scene Recognition: Learning from Scratch Better than Transferring from RGB-CNNs \cite{Song2017DepthCF} 

		\item Research on a Surface Defect Detection Algorithm Based on MobileNet-SSD \cite{Li2018ResearchOA} 

	\end{enumerate}
	\subsection{anchor boxes}
	\begin{enumerate}
		\item
	\end{enumerate}
	\subsection{salient object detection}
	\begin{enumerate}
		\item
	\end{enumerate}
\begin{enumerate}
	\item Weakly Supervised Region Proposal Network and Object Detection \cite{Tang2018WeaklySR} 

	\item Robust and Accurate Object Detection via Adversarial Learning \cite{Chen2021RobustAA} 

	\item Side-Aware Boundary Localization for More Precise Object Detection \cite{Wang2020SideAwareBL} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{contrastive learning}
\begin{enumerate}
	\item A Simple Framework for Contrastive Learning of Visual Representations \cite{Chen2020ASF} 

	\item What makes for good views for contrastive learning \cite{Tian2020WhatMF} 

	\item Supervised Contrastive Learning \cite{Khosla2020SupervisedCL} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{semi-supervised learning}
	\subsection{pseudo-labeling}
	\begin{enumerate}
		\item Pseudo-Label : The Simple and Efficient Semi-Supervised Learning Method for Deep Neural Networks \cite{Lee2013PseudoLabelT} 

	\end{enumerate}
\begin{enumerate}
	\item Unbiased Teacher for Semi-Supervised Object Detection \cite{Liu2021UnbiasedTF} 

	\item S4L: Self-Supervised Semi-Supervised Learning \cite{Zhai2019S4LSS} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{representation learning}
	\subsection{contrastive learning}
	\begin{enumerate}
		\item Understanding Contrastive Representation Learning through Alignment and Uniformity on the Hypersphere \cite{Wang2020UnderstandingCR} 

		\item Representation Learning with Contrastive Predictive Coding \cite{Oord2018RepresentationLW} 

	\end{enumerate}
\begin{enumerate}
	\item Comprehensive Attention Self-Distillation for Weakly-Supervised Object Detection \cite{Huang2020ComprehensiveAS} 

	\item Unsupervised Representation Learning by Predicting Image Rotations \cite{Gidaris2018UnsupervisedRL} 

	\item Understanding image representations by measuring their equivariance and equivalence \cite{Lenc2015UnderstandingIR} 

	\item Unsupervised Feature Learning via Non-parametric Instance Discrimination \cite{Wu2018UnsupervisedFL} 

	\item Unsupervised Object Segmentation by Redrawing \cite{Chen2019UnsupervisedOS} 

	\item Unsupervised Visual Representation Learning by Context Prediction \cite{Doersch2015UnsupervisedVR} 

	\item S4L: Self-Supervised Semi-Supervised Learning \cite{Zhai2019S4LSS} 

	\item Self-Supervised Learning of Pretext-Invariant Representations \cite{Misra2020SelfSupervisedLO} 

	\item Look-Into-Object: Self-Supervised Structure Modeling for Object Recognition \cite{Zhou2020LookIntoObjectSS} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{unsupervised learning}
\begin{enumerate}
	\item
\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{transfer learning}
\begin{enumerate}
	\item What is being transferred in transfer learning? \cite{Neyshabur2020WhatIB} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{weakly-supervised learning}
\begin{enumerate}
	\item Simple Does It: Weakly Supervised Instance and Semantic Segmentation \cite{Khoreva2017SimpleDI} 

	\item Weakly Supervised Learning of Instance Segmentation With Inter-Pixel Relations \cite{Ahn2019WeaklySL} 

	\item Weakly Supervised Region Proposal Network and Object Detection \cite{Tang2018WeaklySR} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{manifold learning}
\begin{enumerate}
	\item Disconnected Manifold Learning for Generative Adversarial Networks \cite{Khayatkhoei2018DisconnectedML} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{metric learning}
\begin{enumerate}
	\item
\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{x-shot learning}
	\subsection{few-shot learning}
	\begin{enumerate}
		\item
	\end{enumerate}
	\subsection{one-shot learning}
	\begin{enumerate}
		\item
	\end{enumerate}
	\subsection{zero-shot learning}
	\begin{enumerate}
		\item Zero-Shot Learning—A Comprehensive Evaluation of the Good, the Bad and the Ugly \cite{Xian2019ZeroShotLC} 

	\end{enumerate}
\begin{enumerate}
\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{differential evolution}
\begin{enumerate}
	\item Differential Evolution: A Survey of the State-of-the-Art \cite{Das2011DifferentialEA} 

	\item Differential Evolution – A Simple and Efficient Heuristic for global Optimization over Continuous Spaces \cite{Storn1997DifferentialE} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{low-level visual processing}
	\subsection{correspondence}
	\begin{enumerate}
		\item
	\end{enumerate}
	\subsection{feature matching}
	\begin{enumerate}
		\item LoFTR: Detector-Free Local Feature Matching with Transformers \cite{Sun2021LoFTRDL} 

	\end{enumerate}
	\subsection{descriptors}
	\begin{enumerate}
		\item
	\end{enumerate}
\begin{enumerate}
\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{dimensionality reduction}
	\subsection{pruning & compression}
	\begin{enumerate}
		\item Structured Pruning of Deep Convolutional Neural Networks \cite{Anwar2017StructuredPO} 

	\end{enumerate}
\begin{enumerate}
	\item A global geometric framework for nonlinear dimensionality reduction. \cite{Tenenbaum2000AGG} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{theory of universal approximation}
\begin{enumerate}
	\item Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds \cite{Chen2019EfficientAO} 

	\item A Universal Approximation Theorem of Deep Neural Networks for Expressing Probability Distributions \cite{Lu2020AUA} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{clustering}
\begin{enumerate}
	\item SPICE: Semantic Pseudo-labeling for Image Clustering \cite{Niu2021SPICESP} 

	\item Deep Spectral Clustering Using Dual Autoencoder Network \cite{Yang2019DeepSC} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{mathematics}
	\subsection{group theory}
		\subsubsection{Lie groups}
		\begin{enumerate}
			\item
		\end{enumerate}
	\begin{enumerate}
	\end{enumerate}
	\subsection{topology}
		\subsubsection{smooth manifolds}
		\begin{enumerate}
			\item
		\end{enumerate}
	\begin{enumerate}
		\item Collared and non-collared manifold boundaries \cite{Baillif2022CollaredAN} 

		\item Some remarks on the size of tubular neighborhoods in contact topology and fillability \cite{Niederkruger2010SomeRO} 

	\end{enumerate}
	\subsection{banach spaces}
	\begin{enumerate}
		\item Smooth bump functions and the geometry of banach spaces \cite{Fry2002SmoothBF} 

	\end{enumerate}
	\subsection{optimal transport}
	\begin{enumerate}
		\item Regularity as Regularization: Smooth and Strongly Convex Brenier Potentials in Optimal Transport \cite{Paty2020RegularityAR} 

	\end{enumerate}
\begin{enumerate}
\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{statistics}
	\subsection{markov chains}
		\subsubsection{monte-carlo}
		\begin{enumerate}
			\item
		\end{enumerate}
	\begin{enumerate}
	\end{enumerate}
\begin{enumerate}
	\item Principal component analysis \cite{Wold1987PrincipalCA} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{surveys}
\begin{enumerate}
	\item Differential Evolution: A Survey of the State-of-the-Art \cite{Das2011DifferentialEA} 

	\item Transformers in Vision: A Survey \cite{Khan2022TransformersIV} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{frameworks & code-bases}
\begin{enumerate}
	\item torch.fx: Practical Program Capture and Transformation for Deep Learning in Python \cite{Reed2021torchfxPP} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{unassigned}
\begin{enumerate}
	\item A Learned Representation For Artistic Style \cite{Dumoulin2017ALR} 

	\item Flows for simultaneous manifold learning and density estimation \cite{Brehmer2020FlowsFS} 

	\item Mode Collapse and Regularity of Optimal Transportation Maps \cite{Lei2019ModeCA} 

	\item Minimax Estimation of Maximum Mean Discrepancy with Radial Kernels \cite{Tolstikhin2016MinimaxEO} 

	\item MixUp as Locally Linear Out-Of-Manifold Regularization \cite{Guo2019MixUpAL} 

	\item Multi-Level Representation Learning for Deep Subspace Clustering \cite{Kheirandishfard2020MultiLevelRL} 

	\item Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks \cite{Hu2020ProvableBO} 

	\item Regularity of the distance function \cite{Foote1984RegularityOT} 

	\item Robust Density Estimation under Besov IPM Losses \cite{Uppal2020RobustDE} 

	\item Trivializations for Gradient-Based Optimization on Manifolds \cite{LezcanoCasado2019TrivializationsFG} 

	\item NCP-VAE: Variational Autoencoders with Noise Contrastive Priors \cite{Aneja2020NCPVAEVA} 

\end{enumerate}
\addtocontents{toc}{\setcounter{tocdepth}{6}}
\section{no-category}
\begin{enumerate}
	\item Verified Uncertainty Calibration \cite{Kumar2019VerifiedUC} 

	\item VOLO: Vision Outlooker for Visual Recognition \cite{Yuan2021VOLOVO} 

	\item The Lovasz-Softmax Loss: A Tractable Surrogate for the Optimization of the Intersection-Over-Union Measure in Neural Networks \cite{Berman2018TheLL} 

	\item Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles \cite{Lakshminarayanan2017SimpleAS} 

	\item Sharpness-Aware Minimization for Efficiently Improving Generalization \cite{Foret2021SharpnessAwareMF} 

	\item Selective Kernel Networks \cite{Li2019SelectiveKN} 

	\item Scaled-YOLOv4: Scaling Cross Stage Partial Network \cite{Wang2021ScaledYOLOv4SC} 

	\item Rethinking Pre-training and Self-training \cite{Zoph2020RethinkingPA} 

	\item Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network \cite{Shi2016RealTimeSI} 

	\item Receptive Field Block Net for Accurate and Fast Object Detection \cite{Liu2018ReceptiveFB} 

	\item Provably Efficient Online Hyperparameter Optimization with Population-Based Bandits \cite{ParkerHolder2020ProvablyEO} 

	\item Pseudo-mask Matters in Weakly-supervised Semantic Segmentation \cite{Li2021PseudomaskMI} 

	\item ReMixMatch: Semi-Supervised Learning with Distribution Matching and Augmentation Anchoring \cite{Berthelot2020ReMixMatchSL} 

	\item Rectifying Pseudo Label Learning via Uncertainty Estimation for Domain Adaptive Semantic Segmentation \cite{Zheng2021RectifyingPL} 

	\item Pseudo-Labeling and Confirmation Bias in Deep Semi-Supervised Learning \cite{Arazo2020PseudoLabelingAC} 

	\item PatchShuffle Regularization \cite{Kang2017PatchShuffleR} 

	\item Pretrained Transformers Improve Out-of-Distribution Robustness \cite{Hendrycks2020PretrainedTI} 

	\item PAG-YOLO: A Portable Attention-Guided YOLO Network for Small Ship Detection \cite{Hu2021PAGYOLOAP} 

	\item Path Aggregation Network for Instance Segmentation \cite{Liu2018PathAN} 

	\item Momentum Contrast for Unsupervised Visual Representation Learning \cite{He2020MomentumCF} 

	\item Moser Flow: Divergence-based Generative Modeling on Manifolds \cite{Rozen2021MoserFD} 

	\item Multi-Similarity Loss With General Pair Weighting for Deep Metric Learning \cite{Wang2019MultiSimilarityLW} 

	\item NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection \cite{Ghiasi2019NASFPNLS} 

	\item Network size and weights size for memorization with two-layers neural networks \cite{Bubeck2020NetworkSA} 

	\item Self-Training With Noisy Student Improves ImageNet Classification \cite{Xie2020SelfTrainingWN} 

	\item No Fuss Distance Metric Learning Using Proxies \cite{MovshovitzAttias2017NoFD} 

	\item Non-Local Neural Networks With Grouped Bilinear Attentional Transforms \cite{Chi2020NonLocalNN} 

	\item Non-local Neural Networks \cite{Wang2018NonlocalNN} 

	\item On the Relationship between Self-Attention and Convolutional Layers \cite{Cordonnier2020OnTR} 

	\item Object Region Mining with Adversarial Erasing: A Simple Classification to Semantic Segmentation Approach \cite{Wei2017ObjectRM} 

	\item Objects as Points \cite{Zhou2019ObjectsAP} 

	\item On Calibration of Modern Neural Networks \cite{Guo2017OnCO} 

	\item On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks \cite{Thulasidasan2019OnMT} 

	\item On Network Design Spaces for Visual Recognition \cite{Radosavovic2019OnND} 

	\item On the Expressive Power of Deep Learning: A Tensor Analysis \cite{Cohen2015OnTE} 

	\item OCGAN: One-Class Novelty Detection Using GANs With Constrained Latent Representations \cite{Perera2019OCGANON} 

	\item Optimal Adaptive and Accelerated Stochastic Gradient Descent \cite{Deng2018OptimalAA} 

	\item Optuna: A Next-generation Hyperparameter Optimization Framework \cite{Akiba2019OptunaAN} 

	\item Modeling Visual Context is Key to Augmenting Object Detection Datasets \cite{Dvornik2018ModelingVC} 

	\item MLP-Mixer: An all-MLP Architecture for Vision \cite{Tolstikhin2021MLPMixerAA} 

	\item MMDetection: Open MMLab Detection Toolbox and Benchmark \cite{Chen2019MMDetectionOM} 

	\item MobileViT: Light-weight, General-purpose, and Mobile-friendly Vision Transformer \cite{Mehta2021MobileViTLG} 

	\item Making Convolutional Networks Shift-Invariant Again \cite{Zhang2019MakingCN} 

	\item Meta Pseudo Labels \cite{Pham2021MetaPL} 

	\item MetaAnchor: Learning to Detect Objects with Customized Anchors \cite{Yang2018MetaAnchorLT} 

	\item MixConv: Mixed Depthwise Convolutional Kernels \cite{Tan2019MixConvMD} 

	\item MixUp as Locally Linear Out-Of-Manifold Regularization \cite{Guo2019MixUpAL} 

	\item Learning Data Augmentation Strategies for Object Detection \cite{Zoph2020LearningDA} 

	\item Learning Depth from Single Monocular Images Using Deep Convolutional Neural Fields \cite{Liu2016LearningDF} 

	\item Learning Efficient Object Detection Models with Knowledge Distillation \cite{Chen2017LearningEO} 

	\item Learning Representations by Maximizing Mutual Information Across Views \cite{Bachman2019LearningRB} 

	\item Learning Transferable Architectures for Scalable Image Recognition \cite{Zoph2018LearningTA} 

	\item Learning to Segment Every Thing \cite{Hu2018LearningTS} 

	\item Learning to Segment Object Candidates \cite{Pinheiro2015LearningTS} 

	\item Learning to Segment via Cut-and-Paste \cite{Remez2018LearningTS} 

	\item Least Squares Generative Adversarial Networks \cite{Mao2017LeastSG} 

	\item Towards Automated Deep Learning: Efficient Joint Neural Architecture and Hyperparameter Search \cite{Zela2018TowardsAD} 

	\item Large Batch Training of Convolutional Networks \cite{You2017LargeBT} 

	\item Do Better ImageNet Models Transfer Better? \cite{Kornblith2019DoBI} 

	\item LambdaNetworks: Modeling Long-Range Interactions Without Attention \cite{Bello2021LambdaNetworksML} 

	\item Invariant Information Clustering for Unsupervised Image Classification and Segmentation \cite{Ji2019InvariantIC} 

	\item Instance-Aware, Context-Focused, and Memory-Efficient Weakly Supervised Object Detection \cite{Ren2020InstanceAwareCA} 

	\item Improvements to Context Based Self-Supervised Learning \cite{Mundhenk2018ImprovementsTC} 

	\item Improved Deep Metric Learning with Multi-class N-pair Loss Objective \cite{Sohn2016ImprovedDM} 

	\item Soft-NMS — Improving Object Detection with One Line of Code \cite{Bodla2017SoftNMSI} 

	\item Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation \cite{Lopes2019ImprovingRW} 

	\item Improving the Robustness of Deep Neural Networks via Stability Training \cite{Zheng2016ImprovingTR} 

	\item InstaBoost: Boosting Instance Segmentation via Probability Map Guided Copy-Pasting \cite{Fang2019InstaBoostBI} 

	\item BoxInst: High-Performance Instance Segmentation with Box Annotations \cite{Tian2021BoxInstHI} 

	\item High-Performance Large-Scale Image Recognition Without Normalization \cite{Brock2021HighPerformanceLI} 

	\item Imbalance Problems in Object Detection: A Review \cite{Oksuz2021ImbalancePI} 

	\item GhostNet: More Features From Cheap Operations \cite{Han2020GhostNetMF} 

	\item Gradient Starvation: A Learning Proclivity in Neural Networks \cite{Pezeshki2020GradientSA} 

	\item Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations \cite{Bietti2019GroupIS} 

	\item FreeAnchor: Learning to Match Anchors for Visual Object Detection \cite{Zhang2019FreeAnchorLT} 

	\item FoveaBox: Beyound Anchor-Based Object Detection \cite{Kong2020FoveaBoxBA} 

	\item Gather-Excite: Exploiting Feature Context in Convolutional Neural Networks \cite{Hu2018GatherExciteEF} 

	\item Generalisation in humans and deep neural networks \cite{Geirhos2018GeneralisationIH} 

	\item Feature Pyramid Networks for Object Detection \cite{Lin2017FeaturePN} 

	\item Filter Response Normalization Layer: Eliminating Batch Dependence in the Training of Deep Neural Networks \cite{Singh2020FilterRN} 

	\item Fast R-CNN \cite{Girshick2015FastR} 

	\item FCOS: Fully Convolutional One-Stage Object Detection \cite{Tian2019FCOSFC} 

	\item Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks \cite{Ren2015FasterRT} 

	\item Distance-IoU Loss: Faster and Better Learning for Bounding Box Regression \cite{Zheng2020DistanceIoULF} 

	\item Fast and Accurate Model Scaling \cite{Dollr2021FastAA} 

	\item Dynamic Group Convolution for Accelerating Convolutional Neural Networks \cite{Su2020DynamicGC} 

	\item Ensemble Adversarial Training: Attacks and Defenses \cite{Tramr2018EnsembleAT} 

	\item Explainable Deep One-Class Classification \cite{Liznerski2021ExplainableDO} 

	\item Efficient Neural Architecture Search via Parameter Sharing \cite{Pham2018EfficientNA} 

	\item EfficientDet: Scalable and Efficient Object Detection \cite{Tan2020EfficientDetSA} 

	\item Emerging Properties in Self-Supervised Vision Transformers \cite{Caron2021EmergingPI} 

	\item Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation \cite{Chen2018EncoderDecoderWA} 

	\item End-to-End Object Detection with Transformers \cite{Carion2020EndtoEndOD} 

	\item Evolving Normalization-Activation Layers \cite{Liu2020EvolvingNL} 

	\item FaceNet: A unified embedding for face recognition and clustering \cite{Schroff2015FaceNetAU} 

	\item Densely Connected Convolutional Networks \cite{Huang2017DenselyCC} 

	\item Designing Network Design Spaces \cite{Radosavovic2020DesigningND} 

	\item On the Performance of Differential Evolution for Hyperparameter Tuning \cite{Schmidt2019OnTP} 

	\item Deep One-Class Classification \cite{Ruff2018DeepOC} 

	\item Deep learning is adaptive to intrinsic dimensionality of model smoothness in anisotropic Besov space \cite{Suzuki2019DeepLI} 

	\item Deep Metric Learning With Tuplet Margin Loss \cite{Yu2019DeepML} 

	\item Deep Networks with Stochastic Depth \cite{Huang2016DeepNW} 

	\item CBAM: Convolutional Block Attention Module \cite{Woo2018CBAMCB} 

	\item CSPNet: A New Backbone that can Enhance Learning Capability of CNN \cite{Wang2020CSPNetAN} 

	\item CenterMask: Real-Time Anchor-Free Instance Segmentation \cite{Lee2020CenterMaskRA} 

	\item Channel Pruning for Accelerating Very Deep Neural Networks \cite{He2017ChannelPF} 

	\item Characterizing signal propagation to close the performance gap in unnormalized ResNets \cite{Brock2021CharacterizingSP} 

	\item Classification is a Strong Baseline for Deep Metric Learning \cite{Zhai2019ClassificationIA} 

	\item Compact Bilinear Pooling \cite{Gao2016CompactBP} 

	\item Concurrent Spatial and Channel Squeeze \& Excitation in Fully Convolutional Networks \cite{Roy2018ConcurrentSA} 

	\item Convolutional Rectifier Networks as Generalized Tensor Decompositions \cite{Cohen2016ConvolutionalRN} 

	\item CornerNet: Detecting Objects as Paired Keypoints \cite{Law2018CornerNetDO} 

	\item CIAN: Cross-Image Affinity Net for Weakly Supervised Semantic Segmentation \cite{Fan2020CIANCA} 

	\item Cyclical Learning Rates for Training Neural Networks \cite{Smith2017CyclicalLR} 

	\item D2Det: Towards High Quality Object Detection and Instance Segmentation \cite{Cao2020D2DetTH} 

	\item Deep Anomaly Detection with Outlier Exposure \cite{Hendrycks2019DeepAD} 

	\item Decoupled Weight Decay Regularization \cite{Loshchilov2019DecoupledWD} 

	\item Deformable DETR: Deformable Transformers for End-to-End Object Detection \cite{Zhu2021DeformableDD} 

	\item Data-Efficient Image Recognition with Contrastive Predictive Coding \cite{Hnaff2020DataEfficientIR} 

	\item Deep Clustering for Unsupervised Learning of Visual Features \cite{Caron2018DeepCF} 

	\item Deep Feature Pyramid Reconfiguration for Object Detection \cite{Kong2018DeepFP} 

	\item CCNet: Criss-Cross Attention for Semantic Segmentation \cite{Huang2019CCNetCA} 

	\item Consistency Regularization for Generative Adversarial Networks \cite{Zhang2020ConsistencyRF} 

	\item AutoAssign: Differentiable Label Assignment for Dense Object Detection \cite{Zhu2020AutoAssignDL} 

	\item Bridging the Gap Between Anchor-Based and Anchor-Free Detection via Adaptive Training Sample Selection \cite{Zhang2020BridgingTG} 

	\item Averaging Weights Leads to Wider Optima and Better Generalization \cite{Izmailov2018AveragingWL} 

	\item Benchmarking Neural Network Robustness to Common Corruptions and Perturbations \cite{Hendrycks2019BenchmarkingNN} 

	\item Bag of Freebies for Training Object Detection Neural Networks \cite{Zhang2019BagOF} 

	\item Bilinear CNNs for Fine-grained Visual Recognition \cite{Lin2015BilinearCF} 

	\item Billion-scale semi-supervised learning for image classification \cite{Yalniz2019BillionscaleSL} 

	\item Bottleneck Transformers for Visual Recognition \cite{Srinivas2021BottleneckTF} 

	\item Bounding boxes for weakly supervised segmentation: Global constraints get close to full supervision \cite{Kervadec2020BoundingBF} 

	\item Bag of Tricks for Image Classification with Convolutional Neural Networks \cite{He2019BagOT} 

	\item Best practices for convolutional neural networks applied to visual document analysis \cite{Simard2003BestPF} 

	\item Big Self-Supervised Models are Strong Semi-Supervised Learners \cite{Chen2020BigSM} 

	\item Attention-Based Dropout Layer for Weakly Supervised Single Object Localization and Semantic Segmentation \cite{Choe2021AttentionBasedDL} 

	\item Attention-guided Context Feature Pyramid Network for Object Detection \cite{Cao2020AttentionguidedCF} 

	\item Augmentation for small object detection \cite{Kisantal2019AugmentationFS} 

	\item Adversarial Complementary Learning for Weakly Supervised Object Localization \cite{Zhang2018AdversarialCL} 

	\item Adversarial Examples Improve Image Recognition \cite{Xie2020AdversarialEI} 

	\item Adversarial Logit Pairing \cite{Kannan2018AdversarialLP} 

	\item Adversarial Training Can Hurt Generalization \cite{Raghunathan2019AdversarialTC} 

	\item Adversarial Training for Free! \cite{Shafahi2019AdversarialTF} 

	\item An Energy and GPU-Computation Efficient Backbone Network for Real-Time Object Detection \cite{Lee2019AnEA} 

	\item Anchor Box Optimization for Object Detection \cite{Zhong2020AnchorBO} 

	\item Anchor DETR: Query Design for Transformer-Based Detector \cite{Wang2021AnchorDQ} 

	\item A survey on Image Data Augmentation for Deep Learning \cite{Shorten2019ASO} 

	\item A survey on addressing high-class imbalance in big data \cite{Leevy2018ASO} 

	\item A systematic study of the class imbalance problem in convolutional neural networks \cite{Buda2018ASS} 

	\item An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale \cite{Dosovitskiy2021AnII} 

	\item A generalized framework for population based training \cite{li2019generalized} 

	\item Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation \cite{wei2018revisiting} 

	\item Calibrating deep neural networks using focal loss \cite{mukhoti2020calibrating} 

	\item A survey on semi-supervised learning \cite{van2020survey} 

	\item Consistency regularization for generative adversarial networks \cite{zhang2019consistency} 

	\item Large scale GAN training for high fidelity natural image synthesis \cite{brock2018large} 

	\item Temporal ensembling for semi-supervised learning \cite{laine2016temporal} 

	\item Training convolutional networks with noisy labels \cite{sukhbaatar2014training} 

	\item What is being transferred in transfer learning? \cite{neyshabur2020being} 

	\item Pretrained transformers improve out-of-distribution robustness \cite{hendrycks2020pretrained} 

	\item Pelee: A real-time object detection system on mobile devices \cite{wang2018pelee} 

	\item Differential evolution: A recent review based on state-of-the-art works \cite{ahmad2021differential} 

	\item Efficient inference in fully connected crfs with gaussian edge potentials \cite{krahenbuhl2011efficient} 

	\item Efficientnetv2: Smaller models and faster training \cite{tan2021efficientnetv2} 

	\item A critical analysis of self-supervision, or what we can learn from a single image \cite{asano2019critical} 

	\item A combinatorial perspective on transfer learning \cite{wang2020combinatorial} 

	\item A large-scale study of representation learning with the visual task adaptation benchmark \cite{zhai2019large} 

	\item Bayesian design of control space for optimal assimilation of observations. Part I: Consistent multiscale formalism \cite{bocquet2011bayesian} 

	\item A system for massively parallel hyperparameter tuning \cite{li2020system} 

	\item Realistic evaluation of deep semi-supervised learning algorithms \cite{oliver2018realistic} 

	\item Virtual adversarial training: a regularization method for supervised and semi-supervised learning \cite{miyato2018virtual} 

	\item Deep clustering with a dynamic autoencoder: From reconstruction towards centroids construction \cite{mrabah2020deep} 

	\item Fixmatch: Simplifying semi-supervised learning with consistency and confidence \cite{sohn2020fixmatch} 

	\item Mixmatch: A holistic approach to semi-supervised learning \cite{berthelot2019mixmatch} 

	\item Deep neural nets with interpolating function as output activation \cite{wang2018deep} 

	\item There are many consistent explanations of unlabeled data: Why you should average \cite{athiwaratkun2018there} 

	\item Interpolation consistency training for semi-supervised learning \cite{verma2019interpolation} 

	\item Charting the right manifold: Manifold mixup for few-shot learning \cite{mangla2020charting} 

	\item Un-mix: Rethinking image mixtures for unsupervised visual representation learning \cite{shen2020mix} 

	\item Deformable convnets v2: More deformable, better results \cite{zhu2019deformable} 

	\item CSPNet: A new backbone that can enhance learning capability of CNN \cite{wang2020cspnet} 

	\item Understanding image representations by measuring their equivariance and equivalence \cite{lenc2015understanding} 

	\item Group normalization \cite{wu2018group} 

	\item Yolov4: Optimal speed and accuracy of object detection \cite{bochkovskiy2020yolov4} 

	\item Generalized intersection over union: A metric and a loss for bounding box regression \cite{rezatofighi2019generalized} 

	\item Clustergan: Latent space clustering in generative adversarial networks \cite{mukherjee2019clustergan} 

	\item NCP-VAE: Variational autoencoders with noise contrastive priors \cite{aneja2020ncp} 

	\item Invariance and stability of deep convolutional representations \cite{bietti2017invariance} 

	\item The large learning rate phase of deep learning: the catapult mechanism \cite{lewkowycz2020large} 

	\item Detnet: A backbone network for object detection \cite{li2018detnet} 

	\item Three factors influencing minima in sgd \cite{jastrzkebski2017three} 

	\item A disciplined approach to neural network hyper-parameters: Part 1--learning rate, batch size, momentum, and weight decay \cite{smith2018disciplined} 

	\item Mish: A self regularized non-monotonic neural activation function \cite{misra2019mish} 

	\item Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) \cite{clevert2015fast} 

	\item Deep Sparse Rectifier Neural Networks \cite{pmlr-v15-glorot11a} 

	\item Generalization in deep learning \cite{kawaguchi2017generalization} 

	\item mixup: Beyond empirical risk minimization \cite{zhang2017mixup} 

	\item Manifold mixup: Better representations by interpolating hidden states \cite{verma2019manifold} 

	\item Cutmix: Regularization strategy to train strong classifiers with localizable features \cite{yun2019cutmix} 

	\item Augmix: A simple data processing method to improve robustness and uncertainty \cite{hendrycks2019augmix} 

	\item Improved regularization of convolutional neural networks with cutout \cite{devries2017improved} 

	\item Dropout: a simple way to prevent neural networks from overfitting \cite{srivastava2014dropout} 

	\item Dropblock: A regularization method for convolutional networks \cite{ghiasi2018dropblock} 

	\item Shakedrop regularization for deep residual learning \cite{yamada2019shakedrop} 

	\item Shake-shake regularization \cite{gastaldi2017shake} 

	\item Autoaugment: Learning augmentation policies from data \cite{cubuk2018autoaugment} 

	\item Fast autoaugment \cite{lim2019fast} 

	\item Circumventing Outliers of AutoAugment with Knowledge Distillation \cite{wei2020circumventing} 

	\item Population based augmentation: Efficient learning of augmentation policy schedules \cite{ho2019population} 

	\item Randaugment: Practical automated data augmentation with a reduced search space \cite{cubuk2020randaugment} 

	\item Spatial transformer networks \cite{jaderberg2015spatial} 

	\item Fully convolutional networks for semantic segmentation \cite{long2015fully} 

	\item U-net: Convolutional networks for biomedical image segmentation \cite{ronneberger2015u} 

	\item CSPNet: A new backbone that can enhance learning capability of cnn \cite{wang2020cspnet} 

	\item Complex-yolo: An euler-region-proposal for real-time 3d object detection on point clouds \cite{simony2018complex} 

	\item Deep learning for generic object detection: A survey \cite{liu2020deep} 

	\item Efficientdet: Scalable and efficient object detection \cite{tan2020efficientdet} 

	\item Faster r-cnn: Towards real-time object detection with region proposal networks \cite{ren2015faster} 

	\item Fast r-cnn \cite{girshick2015fast} 

	\item Mask r-cnn \cite{he2017mask} 

	\item Rich feature hierarchies for accurate object detection and semantic segmentation \cite{girshick2014rich} 

	\item Fcos: Fully convolutional one-stage object detection \cite{tian2019fcos} 

	\item Feature pyramid networks for object detection \cite{lin2017feature} 

	\item Focal loss for dense object detection \cite{lin2017focal} 

	\item Objects as points \cite{zhou2019objects} 

	\item Path aggregation network for instance segmentation \cite{liu2018path} 

	\item Receptive field block net for accurate and fast object detection \cite{liu2018receptive} 

	\item Speed/accuracy trade-offs for modern convolutional object detectors \cite{huang2017speed} 

	\item Ssd: Single shot multibox detector \cite{liu2016ssd} 

	\item Towards multi-class object detection in unconstrained remote sensing imagery \cite{azimi2018towards} 

	\item Unsupervised co-segmentation through region matching \cite{rubio2012unsupervised} 

	\item Unsupervised object discovery and localization in the wild: Part-based matching with bottom-up region proposals \cite{cho2015unsupervised} 

	\item You only look once: Unified, real-time object detection \cite{redmon2016you} 

	\item YOLO9000: better, faster, stronger \cite{redmon2017yolo9000} 

	\item Yolov3: An incremental improvement \cite{redmon2018yolov3} 

	\item YOLOv4: Optimal Speed and Accuracy of Object Detection \cite{bochkovskiy2020yolov4} 

	\item Adaptis: Adaptive instance selection network \cite{sofiiuk2019adaptis} 

	\item Panoptic segmentation \cite{kirillov2019panoptic} 

	\item Weakly-and semi-supervised panoptic segmentation \cite{li2018weakly} 

	\item Upsnet: A unified panoptic segmentation network \cite{xiong2019upsnet} 

	\item Attention-guided unified network for panoptic segmentation \cite{li2019attention} 

	\item Panoptic feature pyramid networks \cite{kirillov2019panoptic} 

	\item PP-YOLO: An Effective and Efficient Implementation of Object Detector \cite{long2020pp} 

	\item SOLOv2: Dynamic, Faster and Stronger \cite{wang2020solov2} 

	\item Solo: Segmenting objects by locations \cite{wang2019solo} 

	\item Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding \cite{kendall2015bayesian} 

	\item Calibrated Adversarial Refinement for Multimodal Semantic Segmentation \cite{kassapis2020calibrated} 

	\item DCNAS: Densely Connected Neural Architecture Search for Semantic Image Segmentation \cite{zhang2020dcnas} 

	\item Object-contextual representations for semantic segmentation \cite{yuan2019object} 

	\item Rethinking atrous convolution for semantic image segmentation \cite{chen2017rethinking} 

	\item The role of context for object detection and semantic segmentation in the wild \cite{mottaghi2014role} 

	\item An uncertain future: Forecasting from static images using variational autoencoders \cite{walker2016uncertain} 

	\item Auto-encoding variational bayes \cite{kingma2013auto} 

	\item Do better imagenet models transfer better? \cite{kornblith2019better} 

	\item How transferable are features in deep neural networks? \cite{yosinski2014transferable} 

	\item Large scale fine-grained categorization and domain-specific transfer learning \cite{cui2018large} 

	\item Syn2real: A new benchmark forsynthetic-to-real visual domain adaptation \cite{peng2018syn2real} 

	\item A theory of learning from different domains \cite{ben2010theory} 

	\item Adversarial discriminative domain adaptation \cite{tzeng2017adversarial} 

	\item Analysis of representations for domain adaptation \cite{ben2007analysis} 

	\item Deep domain confusion: Maximizing for domain invariance \cite{tzeng2014deep} 

	\item Deep visual domain adaptation: A survey \cite{wang2018deep} 

	\item Discriminative Feature Alignment: Improving Transferability of Unsupervised Domain Adaptation by Gaussian-guided Latent Alignment \cite{wang2020discriminative} 

	\item Dlid: Deep learning for domain adaptation by interpolating between domains \cite{chopra2013dlid} 

	\item Domain adaptive faster r-cnn for object detection in the wild \cite{chen2018domain} 

	\item Domain separation networks \cite{bousmalis2016domain} 

	\item Domain-adversarial neural networks \cite{ajakan2014domain} 

	\item Domain-adversarial training of neural networks \cite{ganin2016domain} 

	\item Marginalized denoising autoencoders for domain adaptation \cite{chen2012marginalized} 

	\item Multi-adversarial faster-rcnn for unrestricted object detection \cite{he2019multi} 

	\item Strong-weak distribution alignment for adaptive object detection \cite{saito2019strong} 

	\item Unsupervised domain adaptation by backpropagation \cite{ganin2015unsupervised} 

	\item Argoverse: 3D Tracking and Forecasting with Rich Maps \cite{chang2019argoverse} 

	\item How much real data do we actually need: Analyzing object detection performance using synthetic and real data \cite{nowruzi2019real} 

	\item nuScenes: A multimodal dataset for autonomous driving \cite{caesar2019nuscenes} 

	\item Xception: Deep Learning with Depthwise Separable Convolutions \cite{chollet2016xception} 

	\item Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition \cite{He_2014} 

	\item Pyramidal Convolution: Rethinking Convolutional Neural Networks for Visual Recognition \cite{duta2020pyramidal} 

	\item Rethinking the Inception Architecture for Computer Vision \cite{szegedy2015rethinking} 

	\item Benchmark Analysis of Representative Deep Neural Network Architectures \cite{Bianco_2018} 

	\item Deep Layer Aggregation \cite{yu2017deep} 

	\item Understanding Deep Architectures using a Recursive Convolutional Network \cite{eigen2013understanding} 

	\item Learning Implicitly Recurrent CNNs Through Parameter Sharing \cite{savarese2019learning} 

	\item Group Equivariant Convolutional Networks \cite{cohen2016group} 

	\item Wide Residual Networks \cite{zagoruyko2016wide} 

	\item Striving for Simplicity: The All Convolutional Net \cite{springenberg2014striving} 

	\item FitNets: Hints for Thin Deep Nets \cite{romero2014fitnets} 

	\item CBAM: Convolutional Block Attention Module \cite{woo2018cbam} 

	\item Squeeze-and-Excitation Networks \cite{hu2017squeezeandexcitation} 

	\item PolyNet: A Pursuit of Structural Diversity in Very Deep Networks \cite{zhang2016polynet} 

	\item Maxout Networks \cite{goodfellow2013maxout} 

	\item Highway Networks \cite{srivastava2015highway} 

	\item Doubly Convolutional Neural Networks \cite{zhai2016doubly} 

	\item Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning \cite{szegedy2016inceptionv4} 

	\item Going Deeper with Convolutions \cite{szegedy2014going} 

	\item Imagenet classification with deep convolutional neural networks \cite{Krizhevsky_imagenetclassification} 

	\item Deep Pyramidal Residual Networks \cite{han2016deep} 

	\item MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications \cite{howard2017mobilenets} 

	\item ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices \cite{zhang2017shufflenet} 

	\item Very Deep Convolutional Networks for Large-Scale Image Recognition \cite{simonyan2014deep} 

	\item Aggregated Residual Transformations for Deep Neural Networks \cite{xie2016aggregated} 

	\item Deep Residual Learning for Image Recognition \cite{he2015deep} 

	\item Densely Connected Convolutional Networks \cite{huang2016densely} 

	\item Identity Mappings in Deep Residual Networks \cite{he2016identity} 

	\item EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks \cite{tan2019efficientnet} 

	\item Dense Semantic Correspondence where Every Pixel is a Classifier \cite{bristow2015dense} 

	\item FCSS: Fully Convolutional Self-Similarity for Dense Semantic Correspondence \cite{kim2017fcss} 

	\item Image Patch Matching Using Convolutional Descriptors with Euclidean Distance \cite{melekhov2017image} 

	\item Working hard to know your neighbor's margins: Local descriptor learning loss \cite{mishchuk2017working} 

	\item PARN: Pyramidal Affine Regression Networks for Dense Semantic Correspondence \cite{jeon2018parn} 

	\item Universal Correspondence Network \cite{choy2016universal} 

	\item Colour and illumination in computer vision \cite{Finlayson2018ColourAI} 

	\item Learning multiple layers of features from tiny images \cite{krizhevsky2009learning} 

	\item Multi-digit number recognition from street view imagery using deep convolutional neural networks \cite{goodfellow2013multi} 

	\item Harmonic Networks with Limited Training Samples \cite{ulicny2019harmonic} 

	\item Principles of neural science \cite{kandel2000principles} 

	\item Object Recognition with and without Objects \cite{zhu2016object} 

	\item Deep networks with stochastic depth \cite{huang2016deep} 

	\item A downsampled variant of imagenet as an alternative to the cifar datasets \cite{chrabaszcz2017downsampled} 

	\item The caltech-ucsd birds-200-2011 dataset \cite{wah2011caltech} 

	\item 3d object representations for fine-grained categorization \cite{krause20133d} 

	\item Fine-grained visual classification of aircraft \cite{maji2013fine} 

	\item Imagenet large scale visual recognition challenge \cite{russakovsky2015imagenet} 

	\item Beyond Filters: Compact Feature Map for Portable Deep Model \cite{pmlr-v70-wang17m} 

	\item Compression-aware Training of Deep Networks \cite{alvarez2017compressionaware} 

	\item GhostNet: More Features from Cheap Operations \cite{han2019ghostnet} 

	\item Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation \cite{zhang2019teacher} 

	\item PerforatedCNNs: Acceleration through Elimination of Redundant Convolutions \cite{figurnov2015perforatedcnns} 

	\item Soft-to-Hard Vector Quantization for End-to-End Learning Compressible Representations \cite{agustsson2017softtohard} 

	\item Wasserstein Distance Guided Representation Learning for Domain Adaptation \cite{shen2017wasserstein} 

	\item A Note on the Inception Score \cite{barratt2018note} 

	\item Adversarial Feature Learning \cite{donahue2016adversarial} 

	\item cGANs with Projection Discriminator \cite{miyato2018cgans} 

	\item Conditional Generative Adversarial Nets \cite{mirza2014conditional} 

	\item Conditional Image Synthesis With Auxiliary Classifier GANs \cite{odena2016conditional} 

	\item Context Encoders: Feature Learning by Inpainting \cite{pathak2016context} 

	\item Deep Generative Image Models using a Laplacian Pyramid of Adversarial Networks \cite{denton2015deep} 

	\item Demystifying MMD GANs \cite{bikowski2018demystifying} 

	\item Differentiable Augmentation for Data-Efficient GAN Training \cite{zhao2020differentiable} 

	\item DualGAN: Unsupervised Dual Learning for Image-to-Image Translation \cite{yi2017dualgan} 

	\item Energy-based Generative Adversarial Network \cite{zhao2016energybased} 

	\item f-GAN: Training Generative Neural Samplers using Variational Divergence Minimization \cite{nowozin2016fgan} 

	\item From GAN to WGAN \cite{weng2019gan} 

	\item GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium \cite{heusel2017gans} 

	\item Generalization and Equilibrium in Generative Adversarial Nets (GANs) \cite{arora2017generalization} 

	\item InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets \cite{chen2016infogan} 

	\item SeqGAN: Sequence Generative Adversarial Nets with Policy Gradient \cite{yu2016seqgan} 

	\item Activation Maximization Generative Adversarial Nets \cite{zhou2017activation} 

	\item Triple Generative Adversarial Nets \cite{li2017triple} 

	\item Continual Learning in Generative Adversarial Nets \cite{seff2017continual} 

	\item Dual Discriminator Generative Adversarial Nets \cite{nguyen2017dual} 

	\item Generative Adversarial Networks in Computer Vision: A Survey and Taxonomy \cite{wang2019generative} 

	\item Generative Adversarial Text to Image Synthesis \cite{reed2016generative} 

	\item Generative Modeling by Estimating Gradients of the Data Distribution \cite{song2019generative} 

	\item Generative Moment Matching Networks \cite{li2015generative} 

	\item Image-to-Image Translation with Conditional Adversarial Networks \cite{isola2016imagetoimage} 

	\item Improved Techniques for Training GANs \cite{salimans2016improved} 

	\item Improved Training of Wasserstein GANs \cite{gulrajani2017improved} 

	\item Large Scale GAN Training for High Fidelity Natural Image Synthesis \cite{brock2018large} 

	\item Learning in Implicit Generative Models \cite{mohamed2016learning} 

	\item Least Squares Generative Adversarial Networks \cite{mao2016squares} 

	\item Loss-Sensitive Generative Adversarial Networks on Lipschitz Densities \cite{qi2017losssensitive} 

	\item MMD GAN: Towards Deeper Understanding of Moment Matching Network \cite{li2017mmd} 

	\item Progressive Growing of GANs for Improved Quality, Stability, and Variation \cite{karras2017progressive} 

	\item Self-Attention Generative Adversarial Networks \cite{zhang2018selfattention} 

	\item Self-Supervised GANs via Auxiliary Rotation Loss \cite{chen2018selfsupervised} 

	\item Semantic Image Synthesis with Spatially-Adaptive Normalization \cite{park2019semantic} 

	\item Semi-Supervised Learning with Generative Adversarial Networks \cite{odena2016semisupervised} 

	\item Spectral Normalization for Generative Adversarial Networks \cite{miyato2018spectral} 

	\item Twin Auxiliary Classifiers GAN \cite{gong2019twin} 

	\item Towards Principled Methods for Training Generative Adversarial Networks \cite{arjovsky2017principled} 

	\item Training generative neural networks via Maximum Mean Discrepancy optimization \cite{dziugaite2015training} 

	\item Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks \cite{zhu2017unpaired} 

	\item Unrolled Generative Adversarial Networks \cite{metz2016unrolled} 

	\item Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks \cite{radford2015unsupervised} 

	\item Wasserstein GAN \cite{arjovsky2017wasserstein} 

	\item Sphere Generative Adversarial Network Based on Geometric Moment Matching \cite{Park2019SphereGA} 

	\item On Convergence and Stability of GANs \cite{Kodali2018OnCA} 

	\item Constraint-Aware Deep Neural Network Compression \cite{Chen2018ConstraintAwareDN} 

	\item Distinctive Image Features from Scale-Invariant Keypoints \cite{LoweDavid2004DistinctiveIF} 

	\item SURF: Speeded Up Robust Features \cite{Bay2006SURFSU} 

	\item On Detection of Multiple Object Instances Using Hough Transforms \cite{Barinova2010OnDO} 

	\item Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters \cite{Laguna2019KeyNetKD} 

	\item KAZE Features \cite{Alcantarilla2012KAZEF} 

	\item Beyond Cartesian Representations for Local Descriptors \cite{Ebel2019BeyondCR} 

	\item Anatomy of the SIFT Method \cite{ReyOtero2014AnatomyOT} 

	\item Saliency filters: Contrast based filtering for salient region detection \cite{Perazzi2012SaliencyFC} 

	\item Salient object detection: A survey \cite{Borji2019SalientOD} 

	\item Salient Object Detection in the Deep Learning Era: An In-Depth Survey \cite{Wang2019SalientOD} 

	\item RC-DARTS: Resource Constrained Differentiable Architecture Search \cite{Jin2019RCDARTSRC} 

	\item Neural Architecture Search with Reinforcement Learning \cite{Zoph2017NeuralAS} 

	\item Joint Neural Architecture Search and Quantization \cite{Chen2018JointNA} 

	\item Designing Neural Network Architectures using Reinforcement Learning \cite{Baker2017DesigningNN} 

	\item AutoGAN: Neural Architecture Search for Generative Adversarial Networks \cite{Gong2019AutoGANNA} 

	\item Binding in short-term visual memory. \cite{Wheeler2002BindingIS} 

	\item A Model for Visual Memory Tasks1 \cite{Sperling1963AMF} 

	\item Distributed and Overlapping Representations of Faces and Objects in Ventral Temporal Cortex \cite{Haxby2001DistributedAO} 

	\item Integration of Local Features into Global Shapes Monkey and Human fMRI Studies \cite{Kourtzi2003IntegrationOL} 

	\item Matching Categorical Object Representations in Inferior Temporal Cortex of Man and Monkey \cite{Kriegeskorte2008MatchingCO} 

	\item Object recognition with features inspired by visual cortex \cite{Serre2005ObjectRW} 

	\item Perceptual learning depends on perceptual constancy \cite{Garrigan2008PerceptualLD} 

	\item Robust Object Recognition with Cortex-Like Mechanisms \cite{Serre2007RobustOR} 

	\item Scene Perception in the Human Brain. \cite{Epstein2019ScenePI} 

	\item Visual Object Representation: Interpreting Neurophysiological Data within a Computational Framework \cite{Plaut1990VisualOR} 

	\item A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights \cite{Su2014ADE} 

	\item A disciplined approach to neural network hyper-parameters: Part 1 - learning rate, batch size, momentum, and weight decay \cite{Smith2018ADA} 

	\item A geometric alternative to Nesterov's accelerated gradient descent \cite{Bubeck2015AGA} 

	\item A Kronecker-factored approximate Fisher matrix for convolution layers \cite{Grosse2016AKA} 

	\item Adam: A Method for Stochastic Optimization \cite{kingma2014adam} 

	\item Gradient Descent Provably Optimizes Over-parameterized Neural Networks \cite{Du2019GradientDP} 

	\item Incorporating Nesterov Momentum into Adam \cite{Dozat2016IncorporatingNM} 

	\item Newton Methods for Convolutional Neural Networks \cite{Wang2020NewtonMF} 

	\item Second-order Optimization for Neural Networks \cite{Martens2016SecondorderOF} 

	\item SGDR: Stochastic Gradient Descent with Warm Restarts \cite{Loshchilov2017SGDRSG} 

	\item Stochastic Gradient Descent Tricks \cite{Bottou2012StochasticGD} 

	\item Three Factors Influencing Minima in SGD \cite{Jastrzebski2017ThreeFI} 

	\item Train longer, generalize better: closing the generalization gap in large batch training of neural networks \cite{Hoffer2017TrainLG} 

	\item Understanding the difficulty of training deep feedforward neural networks \cite{Glorot2010UnderstandingTD} 

	\item Variants of RMSProp and Adagrad with Logarithmic Regret Bounds \cite{Mukkamala2017VariantsOR} 

	\item Bridging the Gaps Between Residual Learning, Recurrent Neural Networks and Visual Cortex \cite{Liao2016BridgingTG} 

	\item A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks \cite{Hendrycks2017ABF} 

	\item A guide to convolution arithmetic for deep learning \cite{Dumoulin2016AGT} 

	\item All you need is a good init \cite{Mishkin2016AllYN} 

	\item Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift \cite{Ioffe2015BatchNA} 

	\item Convergence Analysis of Two-layer Neural Networks with ReLU Activation \cite{Li2017ConvergenceAO} 

	\item Convolutional Neural Fabrics \cite{Saxena2016ConvolutionalNF} 

	\item Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification \cite{He2015DelvingDI} 

	\item Do we need hundreds of classifiers to solve real world classification problems? \cite{Delgado2014DoWN} 

	\item Don't Decay the Learning Rate, Increase the Batch Size \cite{Smith2018DontDT} 

	\item Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks \cite{Liang2018EnhancingTR} 

	\item Explaining nonlinear classification decisions with deep Taylor decomposition \cite{Montavon2017ExplainingNC} 

	\item Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains \cite{Tancik2020FourierFL} 

	\item Generalized Linear Models \cite{McCullagh1972GeneralizedLM} 

	\item Maximum Mean Discrepancy Gradient Flow \cite{Arbel2019MaximumMD} 

	\item Minimax Estimation of Maximum Mean Discrepancy with Radial Kernels \cite{Tolstikhin2016MinimaxEO} 

	\item A Kernel Two-Sample Test \cite{Gretton2012AKT} 

	\item Modular Block-diagonal Curvature Approximations for Feedforward Architectures \cite{Dangel2020ModularBC} 

	\item On the empirical estimation of integral probability metrics \cite{Sriperumbudur2012OnTE} 

	\item On the importance of initialization and momentum in deep learning \cite{Sutskever2013OnTI} 

	\item Overcoming Classifier Imbalance for Long-tail Object Detection with Balanced Group Softmax \cite{Li2020OvercomingCI} 

	\item Pix2Vox++: Multi-scale Context-aware 3D Object Reconstruction from Single and Multiple Images \cite{Xie2020Pix2VoxMC} 

	\item Radial basis functions for the multivariate interpolation of large scattered data sets \cite{Lazzaro2002RadialBF} 

	\item The Loss Surface of Deep and Wide Neural Networks \cite{Nguyen2017TheLS} 

	\item Training Neural Networks with Local Error Signals \cite{Nkland2019TrainingNN} 

	\item Unbiased look at dataset bias \cite{Torralba2011UnbiasedLA} 

	\item Understanding the Effective Receptive Field in Deep Convolutional Neural Networks \cite{Luo2016UnderstandingTE} 

	\item Visualizing the Loss Landscape of Neural Nets \cite{Li2018VisualizingTL} 

	\item Kernel Mean Embedding of Distributions: A Review and Beyonds \cite{Muandet2017KernelME} 

	\item A Survey of Semantic Segmentation \cite{Thoma2016ASO} 

	\item A survey of the recent architectures of deep convolutional neural networks \cite{Khan2020ASO} 

	\item A Survey on Metric Learning for Feature Vectors and Structured Data \cite{Bellet2013ASO} 

	\item A Survey on Multi-Task Learning \cite{Zhang2017ASO} 

	\item Automatic differentiation in machine learning: a survey \cite{Baydin2017AutomaticDI} 

	\item Optimization Methods for Large-Scale Machine Learning \cite{Bottou2018OptimizationMF} 

	\item Video Panoptic Segmentation \cite{Kim2020VideoPS} 

	\item Understanding convolutional neural networks with a mathematical model \cite{Kuo2016UnderstandingCN} 

	\item A Mathematical Theory of Deep Convolutional Neural Networks for Feature Extraction \cite{Wiatowski2018AMT} 

	\item A Modern Take on the Bias-Variance Tradeoff in Neural Networks \cite{Neal2018AMT} 

	\item Benefits of Depth in Neural Networks \cite{Telgarsky2016BenefitsOD} 

	\item Diverse Neural Network Learns True Target Functions \cite{Xie2017DiverseNN} 

	\item Exact solutions to the nonlinear dynamics of learning in deep linear neural networks \cite{Saxe2014ExactST} 

	\item Learning Neural Networks with Two Nonlinear Layers in Polynomial Time \cite{Goel2019LearningNN} 

	\item Learning Polynomials with Neural Networks \cite{Andoni2014LearningPW} 

	\item Multilayer Feedforward Networks with a Non-Polynomial Activation Function Can Approximate Any Function \cite{Leshno1993MultilayerFN} 

	\item Neural Tangent Kernel: Convergence and Generalization in Neural Networks \cite{Jacot2018NeuralTK} 

	\item On the Number of Linear Regions of Deep Neural Networks \cite{Montfar2014OnTN} 

	\item Provable Bounds for Learning Some Deep Representations \cite{Arora2014ProvableBF} 

	\item Recovery Guarantees for One-hidden-layer Neural Networks \cite{Zhong2017RecoveryGF} 

	\item Shallow vs. Deep Sum-Product Networks \cite{Delalleau2011ShallowVD} 

	\item The Expressive Power of Neural Networks: A View from the Width \cite{Lu2017TheEP} 

	\item The Power of Depth for Feedforward Neural Networks \cite{Eldan2016ThePO} 

	\item Optimization for Machine Learning \cite{10.5555/2051759} 

	\item Characterizing the Universal Approximation Property \cite{Kratsios2019CharacterizingTU} 

	\item When and Why Are Deep Networks Better Than Shallow Ones? \cite{Mhaskar2017WhenAW} 

	\item Understanding Machine Learning: From Theory To Algorithms \cite{David2015UnderstandingML} 

	\item Computer Vision: A Modern Approach \cite{Forsyth2002ComputerVA} 

	\item An Invitation to 3-D Vision: From Images to Geometric Models \cite{Ma2004AnIT} 

	\item Computer Vision - Algorithms and Applications \cite{Szeliski2011ComputerV} 

	\item Introducing Monte Carlo methods with \cite{Robert2013IntroducingMC} 

	\item Monte Carlo Sampling Methods Using Markov Chains and Their Applications \cite{Hastings1970MonteCS} 

	\item Applied Stochastic Processes \cite{Liao2013AppliedSP} 

	\item Deep Learning \cite{Goodfellow-et-al-2016} 

	\item Dive into Deep Learning \cite{zhang2020dive} 

	\item Machine learning - a probabilistic perspective \cite{Murphy2012MachineL} 

	\item ShuffleNet V2: Practical Guidelines for Efficient CNN Architecture Design \cite{Ma2018ShuffleNetVP} 

	\item The History Began from AlexNet: A Comprehensive Survey on Deep Learning Approaches \cite{Alom2018TheHB} 

	\item Adversarial Examples Are Not Bugs, They Are Features \cite{Ilyas2019AdversarialEA} 

	\item Asymptotic Guarantees for Learning Generative Models with the Sliced-Wasserstein Distance \cite{Nadjahi2019AsymptoticGF} 

	\item Attention is All you Need \cite{Vaswani2017AttentionIA} 

	\item Attention U-Net: Learning Where to Look for the Pancreas \cite{Oktay2018AttentionUL} 

	\item Generalized Sliced Wasserstein Distances \cite{Kolouri2019GeneralizedSW} 

	\item Regularized Evolution for Image Classifier Architecture Search \cite{Real2019RegularizedEF} 

	\item Residual Networks Behave Like Ensembles of Relatively Shallow Networks \cite{Veit2016ResidualNB} 

	\item Swapout: Learning an ensemble of deep architectures \cite{Singh2016SwapoutLA} 

	\item Mobilenetv2: Inverted residuals and linear bottlenecks \cite{sandler2018mobilenetv2} 

	\item Searching for mobilenetv3 \cite{howard2019searching} 

	\item Resnet in Resnet: Generalizing Residual Architectures \cite{Targ2016ResnetIR} 

	\item Residual Networks of Residual Networks: Multilevel Residual Networks \cite{Zhang2018ResidualNO} 

	\item Generative adversarial nets \cite{goodfellow2014generative} 

	\item Bag of tricks for image classification with convolutional neural networks \cite{he2019bag} 

	\item Auto-Encoding Variational Bayes \cite{Kingma2014AutoEncodingVB} 

	\item Tutorial on Variational Autoencoders \cite{Doersch2016TutorialOV} 

	\item InfoVAE: Information Maximizing Variational Autoencoders \cite{zhao2017infovae} 

	\item Learning Representations by Maximizing Mutual Information in Variational Autoencoders \cite{LotfiRezaabad2020LearningRB} 

	\item MAE: Mutual Posterior-Divergence Regularization for Variational AutoEncoders \cite{Ma2019MAEMP} 

	\item Disentangling by Factorising \cite{Kim2018DisentanglingBF} 

	\item Elbo surgery: yet another way to carve up the variational evidence lower bound \cite{hoffman2016elbo} 

	\item Recent Advances in Autoencoder-Based Representation Learning \cite{Tschannen2018RecentAI} 

	\item Variational Inference: A Review for Statisticians \cite{Blei2016VariationalIA} 

	\item Stochastic Backpropagation and Approximate Inference in Deep Generative Models \cite{Rezende2014StochasticBA} 

	\item Learning Independent Features with Adversarial Nets for Non-linear ICA \cite{Brakel2018LearningIF} 

	\item Mutual Information Neural Estimation \cite{Belghazi2018MutualIN} 

	\item Adversarially Learned Inference \cite{Dumoulin2017AdversariallyLI} 

	\item Emergence of Invariance and Disentanglement in Deep Representations \cite{Achille2018EmergenceOI} 

	\item Understanding the limitations of variational mutual information estimators \cite{song2019understanding} 

	\item Information, divergence and risk for binary experiments \cite{reid2011information} 

	\item Extracting and composing robust features with denoising autoencoders \cite{vincent2008extracting} 

	\item Representation Learning: A Review and New Perspectives \cite{Bengio2013RepresentationLA} 

	\item Deep High-Resolution Representation Learning for Visual Recognition \cite{Wang2020DeepHR} 

	\item Stacked Generative Adversarial Networks \cite{Huang2017StackedGA} 

	\item The IM Algorithm: A Variational Approach to Information Maximization \cite{Barber2003TheIA} 

	\item FineGAN: Unsupervised Hierarchical Disentanglement for Fine-Grained Object Generation and Discovery \cite{Singh2019FineGANUH} 

	\item Group equivariant convolutional networks \cite{cohen2016group} 

\end{enumerate}
\newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{bibliography.bib}
\end{document}